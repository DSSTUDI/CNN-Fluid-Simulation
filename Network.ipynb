{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process shapes data\n",
    "raw_shapes = []\n",
    "directory = 'Dataset/shape'\n",
    "for file in sorted(os.listdir(directory)):\n",
    "    filepath = os.path.join(directory, file)\n",
    "    with open(filepath, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "        raw_shapes.append(data)\n",
    "\n",
    "clean_shapes = []\n",
    "for shape in raw_shapes:\n",
    "    clean_shape = pd.DataFrame(shape).iloc[1:-1, 1:-2]\n",
    "    clean_shapes.append(clean_shape.transpose())\n",
    "\n",
    "# Create signed distance fields for shapes\n",
    "sdf_shapes = []\n",
    "for s in clean_shapes:\n",
    "    # Create a mask for the inside and outside of the shape\n",
    "    shape = s.astype(float)\n",
    "    inside_mask = shape == -1\n",
    "    outside_mask = shape == 0\n",
    "\n",
    "    # Compute the distance transform for both the inside and outside\n",
    "    distance_inside = distance_transform_edt(inside_mask)\n",
    "    distance_outside = distance_transform_edt(outside_mask)\n",
    "\n",
    "    # Create the signed distance field\n",
    "    signed_distance_field = distance_outside - distance_inside\n",
    "\n",
    "    # Normalize the signed distance field\n",
    "    max_distance = np.max(np.abs(signed_distance_field))\n",
    "    if max_distance > 0:\n",
    "        signed_distance_field = signed_distance_field / max_distance\n",
    "\n",
    "    # Set the values inside the original shape to 0\n",
    "    signed_distance_field[inside_mask] = 0\n",
    "    sdf_shapes.append(signed_distance_field)\n",
    "\n",
    "\n",
    "# Process vertical data\n",
    "raw_verticals = []\n",
    "directory = 'Dataset/vertical'\n",
    "for file in sorted(os.listdir(directory)):\n",
    "    filepath = os.path.join(directory, file)\n",
    "    with open(filepath, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "        raw_verticals.append(data)\n",
    "\n",
    "clean_verticals = []\n",
    "for vertical in raw_verticals:\n",
    "    clean_vertical = pd.DataFrame(vertical).iloc[1:-1, 1:-2]\n",
    "    clean_verticals.append(clean_vertical.transpose())\n",
    "\n",
    "\n",
    "# Process horizontal data\n",
    "raw_horizontals = []\n",
    "directory = 'Dataset/horizontal'\n",
    "for file in sorted(os.listdir(directory)):\n",
    "    filepath = os.path.join(directory, file)\n",
    "    with open(filepath, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "        raw_horizontals.append(data)\n",
    "\n",
    "clean_horizontals = []\n",
    "for horizontal in raw_horizontals:\n",
    "    clean_horizontal = pd.DataFrame(horizontal).iloc[1:-1, 1:-2]\n",
    "    clean_horizontals.append(clean_horizontal.transpose())\n",
    "\n",
    "\n",
    "# Calculate total velocity\n",
    "total_velocities = []\n",
    "for i in range(len(clean_shapes)):\n",
    "    total_velocity = np.sqrt(np.square(clean_verticals[i].astype(float)) + np.square(clean_horizontals[i].astype(float)))\n",
    "    total_velocities.append(total_velocity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/41xxgkw13dj0b75snqswj2r00000gn/T/ipykernel_44714/2860088081.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  data_tensor = torch.FloatTensor(temp)\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset\n",
    "temp = []\n",
    "for i in range(len(sdf_shapes)):\n",
    "    input_example = sdf_shapes[i].astype(float)\n",
    "    output_example = total_velocities[i].values.astype(float)\n",
    "    temp.append((input_example, output_example))\n",
    "\n",
    "data_tensor = torch.FloatTensor(temp)\n",
    "\n",
    "# Split training dataset into training, validation, and test\n",
    "training_size = int(0.6 * len(data_tensor))\n",
    "validation_size = int(0.2 * len(data_tensor))\n",
    "test_size = len(data_tensor) - training_size - validation_size\n",
    "\n",
    "training_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(data_tensor, [training_size, validation_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFDNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CFDNet, self).__init__()\n",
    "\n",
    "        # Input: [batch_size, 1, 32, 32]\n",
    "        \n",
    "        # Convolutional layers\n",
    "        # Output size: [batch_size, 128, 16, 16]\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=128, kernel_size=(4, 4), stride=2, padding=1)\n",
    "        \n",
    "        # Output size: [batch_size, 512, 8, 8]\n",
    "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=512, kernel_size=(4, 4), stride=2, padding=1)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        # Flattened input size: 512 * 8 * 8 = 32768\n",
    "        # Output size: 512 * 4 * 4 = 8192\n",
    "        self.fc1 = nn.Linear(in_features=512*8*8, out_features=512*4*4)\n",
    "        \n",
    "        # Deconvolutional layers for x-component\n",
    "        # Output size: [batch_size, 256, 8, 8]\n",
    "        self.deconv1_x = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=(4, 4), stride=2, padding=1)\n",
    "        \n",
    "        # Output size: [batch_size, 128, 16, 16]\n",
    "        self.deconv2_x = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(4, 4), stride=2, padding=1)\n",
    "        \n",
    "        # Output size: [batch_size, 64, 32, 32]\n",
    "        self.deconv3_x = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=(4, 4), stride=2, padding=1)\n",
    "        \n",
    "        # Output size: [batch_size, 1, 32, 32]\n",
    "        self.deconv4_x = nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=(3, 3), stride=1, padding=1)\n",
    "\n",
    "        # Deconvolutional layers for y-component\n",
    "        # Output size: [batch_size, 256, 8, 8]\n",
    "        self.deconv1_y = nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=(4, 4), stride=2, padding=1)\n",
    "        \n",
    "        # Output size: [batch_size, 128, 16, 16]\n",
    "        self.deconv2_y = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=(4, 4), stride=2, padding=1)\n",
    "        \n",
    "        # Output size: [batch_size, 64, 32, 32]\n",
    "        self.deconv3_y = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=(4, 4), stride=2, padding=1)\n",
    "        \n",
    "        # Output size: [batch_size, 1, 32, 32]\n",
    "        self.deconv4_y = nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=(3, 3), stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input: [batch_size, 1, 32, 32]\n",
    "\n",
    "        # Convolutional layers\n",
    "        x_conv = F.relu(self.conv1(x))  # Output: [batch_size, 128, 16, 16]\n",
    "        x_conv = F.relu(self.conv2(x_conv))  # Output: [batch_size, 512, 8, 8]\n",
    "\n",
    "        # Flatten and Fully connected layer\n",
    "        x_fc = x_conv.view(x_conv.size(0), 512*8*8)  # Flatten to 32768 elements\n",
    "        x_fc = F.relu(self.fc1(x_fc))  # Output: 8192 elements\n",
    "\n",
    "        # Reshape for deconvolution\n",
    "        x_reshaped = x_fc.view(x.size(0), 512, 4, 4)  # Reshape to [batch_size, 512, 4, 4]\n",
    "\n",
    "        # Deconvolution path for x-component\n",
    "        x_out = F.relu(self.deconv1_x(x_reshaped))  # Output: [batch_size, 256, 8, 8]\n",
    "        x_out = F.relu(self.deconv2_x(x_out))  # Output: [batch_size, 128, 16, 16]\n",
    "        x_out = F.relu(self.deconv3_x(x_out))  # Output: [batch_size, 64, 32, 32]\n",
    "        x_out = torch.sigmoid(self.deconv4_x(x_out))  # Output: [batch_size, 1, 32, 32]\n",
    "\n",
    "        # Deconvolution path for y-component\n",
    "        y_out = F.relu(self.deconv1_y(x_reshaped))  # Output: [batch_size, 256, 8, 8]\n",
    "        y_out = F.relu(self.deconv2_y(y_out))  # Output: [batch_size, 128, 16, 16]\n",
    "        y_out = F.relu(self.deconv3_y(y_out))  # Output: [batch_size, 64, 32, 32]\n",
    "        y_out = torch.sigmoid(self.deconv4_y(y_out))  # Output: [batch_size, 1, 32, 32]\n",
    "\n",
    "        # Element-wise multiplication with the original input resized to match the final output dimensions\n",
    "        x_resized = F.interpolate(x, size=(32, 32))  # Ensure x is resized to [batch_size, 1, 32, 32]\n",
    "        x_component = x_out * x_resized  # Element-wise multiplication\n",
    "\n",
    "        y_resized = F.interpolate(x, size=(32, 32))  # Ensure x is resized to [batch_size, 1, 32, 32]\n",
    "        y_component = y_out * y_resized  # Element-wise multiplication\n",
    "\n",
    "        return x_component, y_component\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# model = CFDNet()\n",
    "# input_tensor = torch.randn(1, 1, 32, 32)\n",
    "# x_output, y_output = model(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = CFDNet()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(total_output, tensor_labels)\n\u001b[1;32m     18\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 19\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# accumulate the running loss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/rmsprop.py:120\u001b[0m, in \u001b[0;36mRMSprop.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    116\u001b[0m     momentum_buffer_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    118\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(group, params_with_grad, grads, square_avgs, momentum_buffer_list, grad_avgs)\n\u001b[0;32m--> 120\u001b[0m     \u001b[43mrmsprop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43msquare_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malpha\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcentered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcentered\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/rmsprop.py:237\u001b[0m, in \u001b[0;36mrmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, foreach, maximize, differentiable, has_complex, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_rmsprop\n\u001b[0;32m--> 237\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43msquare_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcentered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcentered\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/optim/rmsprop.py:301\u001b[0m, in \u001b[0;36m_single_tensor_rmsprop\u001b[0;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, lr, alpha, eps, weight_decay, momentum, centered, maximize, differentiable, has_complex)\u001b[0m\n\u001b[1;32m    299\u001b[0m     avg \u001b[38;5;241m=\u001b[39m avg\u001b[38;5;241m.\u001b[39madd(eps)\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 301\u001b[0m     avg \u001b[38;5;241m=\u001b[39m \u001b[43mavg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m momentum \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    304\u001b[0m     buf \u001b[38;5;241m=\u001b[39m momentum_buffer_list[i]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for data in training_dataset:\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        tensor_inputs = torch.FloatTensor(inputs).reshape(1, 1, 32, 32)\n",
    "        tensor_labels = torch.FloatTensor(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        x_output, y_output = model(tensor_inputs)\n",
    "        total_output = torch.sqrt(torch.square(x_output) + torch.square(y_output))\n",
    "        loss = criterion(total_output, tensor_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate the running loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # print statistics after processing the entire dataset\n",
    "    print('[%d] loss: %.3f' % (epoch + 1, running_loss / len(training_dataset)))\n",
    "    running_loss = 0.0  # Reset running loss after each epoch\n",
    "\n",
    "print('Finished Training')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
